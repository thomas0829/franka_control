{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data ...\n",
      "Loading data/left_25 train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 146.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data/right_25 train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 160.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data/middle_25 train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 152.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data/left_25 eval ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 157.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data/right_25 eval ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 153.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data/middle_25 eval ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 148.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing training statistics ...\n",
      "Normalizing actions ...\n",
      "Saved at: data/left_right_middle_75\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "import datetime\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "blocking_control = True\n",
    "normalize_acts = True\n",
    "normalize_obs = False\n",
    "\n",
    "def shortest_angle(angles):\n",
    "    return (angles + np.pi) % (2*np.pi) - np.pi\n",
    "\n",
    "def normalize(arr, stats, key):\n",
    "    min_val, max_val = stats[f\"{key}_min\"], stats[f\"{key}_max\"]\n",
    "    return 2 * (arr - min_val) / (max_val - min_val) - 1\n",
    "\n",
    "def unnormalize(arr, stats, key):\n",
    "    min_val, max_val = stats[f\"{key}_min\"], stats[f\"{key}_max\"]\n",
    "    return 0.5 * (arr + 1) * (max_val - min_val) + min_val\n",
    "\n",
    "# create dataset\n",
    "data_dir = \"data\"\n",
    "input_datasets = [\"left_25\", \"right_25\", \"middle_25\"]\n",
    "splits = [\"train\", \"eval\"]\n",
    "output_dataset = \"left_right_middle_75\"\n",
    "dataset_paths = [os.path.join(data_dir, dataset_name) for dataset_name in input_datasets]\n",
    "\n",
    "print(\"Processing data ...\")\n",
    "\n",
    "hdf5_path = os.path.join(data_dir, output_dataset)\n",
    "os.makedirs(hdf5_path, exist_ok=True)\n",
    "f = h5py.File(os.path.join(hdf5_path, \"demos.hdf5\"), \"w\")\n",
    "\n",
    "# create data group\n",
    "grp = f.create_group(\"data\")\n",
    "grp_mask = f.create_group(\"mask\")\n",
    "\n",
    "episodes = 0\n",
    "\n",
    "demo_keys = {}\n",
    "\n",
    "for split in splits:\n",
    "    \n",
    "    demo_keys[split] = []\n",
    "\n",
    "    for dataset_path in dataset_paths:\n",
    "\n",
    "        print(f\"Loading {dataset_path} {split} ...\")\n",
    "        \n",
    "        # gather filenames\n",
    "        file_names = glob.glob(os.path.join(dataset_path, split,\"episode_*.npy\"))\n",
    "        \n",
    "        for i in trange(len(file_names)):\n",
    "\n",
    "            # load data\n",
    "            data = np.load(file_names[i], allow_pickle=True)\n",
    "\n",
    "            # stack data\n",
    "            dic = {}\n",
    "            obs_keys = data[0].keys()\n",
    "            for key in obs_keys:\n",
    "                dic[key] = np.stack([d[key] for d in data])\n",
    "            actions = np.stack([d[\"action\"] for d in data])\n",
    "\n",
    "            if blocking_control:\n",
    "                # compute actual deltas s_t+1 - s_t (keep gripper actions)\n",
    "                actions_tmp = actions.copy()\n",
    "                actions_tmp[:-1,...,:6] = dic[\"lowdim_ee\"][1:,...,:6] - dic[\"lowdim_ee\"][:-1,...,:6]\n",
    "                actions = actions_tmp[:-1]\n",
    "                \n",
    "                # remove last state s_T \n",
    "                for key in obs_keys:\n",
    "                    dic[key] = dic[key][:-1]\n",
    "\n",
    "            # create demo group\n",
    "            demo_key = f\"demo_{episodes}\"\n",
    "            demo_keys[split].append(demo_key)\n",
    "            ep_data_grp = grp.create_group(demo_key)\n",
    "\n",
    "            # compute shortest angle\n",
    "            actions[...,3:6] = shortest_angle(actions[...,3:6])\n",
    "\n",
    "            # add action dataset\n",
    "            ep_data_grp.create_dataset(\"actions\", data=actions)\n",
    "            \n",
    "            # add done dataset\n",
    "            dones = np.zeros(len(actions)).astype(bool)\n",
    "            dones[-1] = True\n",
    "            ep_data_grp.create_dataset(\"dones\", data=dones)\n",
    "\n",
    "            # create obs and next_obs groups\n",
    "            ep_obs_grp = ep_data_grp.create_group(\"obs\")\n",
    "\n",
    "            # add obs and next_obs datasets\n",
    "            for obs_key in obs_keys:\n",
    "                if obs_key == \"language_instruction\":\n",
    "                    continue\n",
    "                obs = dic[obs_key]\n",
    "                ep_obs_grp.create_dataset(obs_key, data=obs)\n",
    "\n",
    "            ep_data_grp.attrs[\"num_samples\"] = len(actions)\n",
    "\n",
    "            episodes += 1\n",
    "\n",
    "    # create mask dataset\n",
    "    grp_mask.create_dataset(split, data=np.array(demo_keys[split], dtype=\"S\"))\n",
    "\n",
    "# write dataset attributes (metadata)\n",
    "grp.attrs[\"episodes\"] = episodes\n",
    "grp.attrs[\"env_args\"] = \"blub\"\n",
    "\n",
    "print(\"Computing training statistics ...\")\n",
    "actions = np.concatenate([grp[demo_key][\"actions\"] for demo_key in demo_keys[\"train\"]])\n",
    "\n",
    "stats = {}\n",
    "stats[\"actions_min\"] = actions.min(axis=0)\n",
    "stats[\"actions_max\"] = actions.max(axis=0)\n",
    "stats_grp = grp.create_group(\"stats\")\n",
    "for key in stats.keys():\n",
    "    stats_grp.create_dataset(key, data=stats[key])\n",
    "\n",
    "print(\"Normalizing actions ...\")\n",
    "for split in splits:\n",
    "    for demo_key in demo_keys[split]:\n",
    "        actions = grp[demo_key][\"actions\"]\n",
    "        actions = normalize(actions, stats, key=\"actions\")\n",
    "        grp[demo_key][\"actions\"][...] = actions\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "grp.attrs[\"date\"] = \"{}-{}-{}\".format(now.month, now.day, now.year)\n",
    "grp.attrs[\"time\"] = \"{}:{}:{}\".format(now.hour, now.minute, now.second)\n",
    "grp.attrs[\"blocking_control\"] = blocking_control\n",
    "\n",
    "f.close()\n",
    "\n",
    "print(\"Saved at: {}\".format(hdf5_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'actions_min': array([-0.00635335, -0.00486431, -0.01493801, -0.02185639, -0.03439584,\n",
       "        -0.02118944,  0.        ]),\n",
       " 'actions_max': array([0.00385384, 0.01571385, 0.01000646, 0.01401203, 0.02496785,\n",
       "        0.01949651, 1.        ])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
