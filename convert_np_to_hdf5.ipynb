{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data and compute stats ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 143.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 35.46it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved at: data/green_block_sl_10_left_blocking/demos.hdf5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "import datetime\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "normalize_acts = True\n",
    "normalize_obs = False\n",
    "\n",
    "def shortest_angle(angles):\n",
    "    return (angles + np.pi) % (2*np.pi) - np.pi\n",
    "\n",
    "def normalize(arr, stats, key):\n",
    "    min_val, max_val = stats[f\"{key}_min\"], stats[f\"{key}_max\"]\n",
    "    return 2 * (arr - min_val) / (max_val - min_val) - 1\n",
    "\n",
    "def unnormalize(arr, stats, key):\n",
    "    min_val, max_val = stats[f\"{key}_min\"], stats[f\"{key}_max\"]\n",
    "    return 0.5 * (arr + 1) * (max_val - min_val) + min_val\n",
    "\n",
    "# create dataset\n",
    "dataset_path = \"data/green_block_sl_10_left_blocking/\"\n",
    "\n",
    "print(\"Loading data and compute stats ...\")\n",
    "# compute action min and max\n",
    "file_names = glob.glob(f\"{dataset_path}/train/episode_*.npy\")\n",
    "data = []\n",
    "for file in tqdm(file_names):\n",
    "    data.append(np.load(file, allow_pickle=True))\n",
    "\n",
    "stats = {}\n",
    "\n",
    "actions = np.concatenate([[np.stack(d[\"action\"]) for d in sample] for sample in data])\n",
    "actions[...,3:6] = shortest_angle(actions[...,3:6])\n",
    "stats[\"actions_min\"] = actions.min(axis=0)\n",
    "stats[\"actions_max\"] = actions.max(axis=0)\n",
    "del actions\n",
    "\n",
    "for key in [\"lowdim_ee\", \"lowdim_qpos\"]:\n",
    "    stack = np.concatenate([[np.stack(d[key]) for d in sample] for sample in data])\n",
    "    stats[f\"{key}_min\"] = stack.min(axis=0)\n",
    "    stats[f\"{key}_max\"] = stack.max(axis=0)\n",
    "    del stack\n",
    "\n",
    "print(\"Processing data ...\")\n",
    "hdf5_path = os.path.join(dataset_path, \"demos.hdf5\")\n",
    "f = h5py.File(hdf5_path, \"w\")\n",
    "\n",
    "# create data group\n",
    "grp = f.create_group(\"data\")\n",
    "grp_mask = f.create_group(\"mask\")\n",
    "\n",
    "episodes = 0\n",
    "\n",
    "for split in [\"train\", \"eval\"]:\n",
    "\n",
    "    # gather filenames\n",
    "    file_names = glob.glob(os.path.join(dataset_path, split,\"episode_*.npy\"))\n",
    "    demo_keys = []\n",
    "\n",
    "    for i in trange(len(file_names)):\n",
    "\n",
    "        # load data\n",
    "        data = np.load(file_names[i], allow_pickle=True)\n",
    "\n",
    "        # stack data\n",
    "        dic = {}\n",
    "        obs_keys = data[0].keys()\n",
    "        for key in obs_keys:\n",
    "            dic[key] = np.stack([d[key] for d in data])\n",
    "        actions = np.stack([d[\"action\"] for d in data])\n",
    "\n",
    "        # create demo group\n",
    "        demo_key = f\"demo_{episodes}\"\n",
    "        demo_keys.append(demo_key)\n",
    "        ep_data_grp = grp.create_group(demo_key)\n",
    "\n",
    "        # compute shortest angle\n",
    "        actions[...,3:6] = shortest_angle(actions[...,3:6])\n",
    "        # normalize -> [-1,1]\n",
    "        if normalize_acts:\n",
    "            actions = normalize(actions, stats, key=\"actions\")\n",
    "\n",
    "        # add action dataset\n",
    "        ep_data_grp.create_dataset(\"actions\", data=actions)\n",
    "        \n",
    "        # add done dataset\n",
    "        dones = np.zeros(len(actions)).astype(bool)\n",
    "        dones[-1] = True\n",
    "        ep_data_grp.create_dataset(\"dones\", data=dones)\n",
    "\n",
    "        # create obs and next_obs groups\n",
    "        ep_obs_grp = ep_data_grp.create_group(\"obs\")\n",
    "        # ep_next_obs_grp = ep_data_grp.create_group(\"next_obs\")\n",
    "\n",
    "        # add obs and next_obs datasets\n",
    "        for obs_key in obs_keys:\n",
    "            if obs_key == \"language_instruction\":\n",
    "                continue\n",
    "            obs = dic[obs_key]\n",
    "            # normalize -> [-1,1]\n",
    "            if obs_key in stats.keys() and normalize_obs:\n",
    "                obs = normalize(obs, stats, key=obs_key)\n",
    "            ep_obs_grp.create_dataset(obs_key, data=obs)\n",
    "            # ep_obs_grp.create_dataset(obs_key, data=obs[:-1])\n",
    "            # ep_next_obs_grp.create_dataset(obs_key, data=obs[1:])\n",
    "\n",
    "        ep_data_grp.attrs[\"num_samples\"] = len(actions)\n",
    "\n",
    "        episodes += 1\n",
    "\n",
    "    # create mask dataset\n",
    "    grp_mask.create_dataset(split, data=np.array(demo_keys, dtype=\"S\"))\n",
    "\n",
    "# write dataset attributes (metadata)\n",
    "grp.attrs[\"episodes\"] = episodes\n",
    "grp.attrs[\"env_args\"] = '{\"env_type\":  \"blub\", \"type\": \"blub\"}'\n",
    "grp.attrs[\"type\"] = \"blub\"\n",
    "\n",
    "# stats_grp = grp.create_group(\"stats\")\n",
    "# for key in stats.keys():\n",
    "#     stats_grp.create_dataset(key, data=stats[key])\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "grp.attrs[\"date\"] = \"{}-{}-{}\".format(now.month, now.day, now.year)\n",
    "grp.attrs[\"time\"] = \"{}:{}:{}\".format(now.hour, now.minute, now.second)\n",
    "\n",
    "f.close()\n",
    "\n",
    "print(\"Saved at: {}\".format(hdf5_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.0182134 , -0.00848135, -0.0281606 , -0.01132216, -0.0264072 ,\n",
       "        -0.02209388,  0.        ]),\n",
       " array([0.00573581, 0.01643882, 0.04378366, 0.02961261, 0.01529887,\n",
       "        0.04490828, 1.        ]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats[\"actions_min\"], stats[\"actions_max\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
