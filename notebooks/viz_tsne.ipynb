{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4643/1761702750.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "# from tsnecuda import TSNE\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_and_images(model, dataloader, embedding):\n",
    "    features = []\n",
    "    labels = []\n",
    "    images = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, batch_labels in tqdm(dataloader):\n",
    "\n",
    "            pil_images = [transforms.ToPILImage()(img_tensor).convert(\"RGB\") for img_tensor in inputs]\n",
    "            if embedding == \"clip\":\n",
    "              # Convert tensor to PIL for CLIP preprocessing\n",
    "              # Preprocess images and move to the correct device\n",
    "              clip_inputs = torch.stack([preprocess(img) for img in pil_images]).to(device)\n",
    "              outputs = model.encode_image(clip_inputs)\n",
    "            else:\n",
    "              outputs = model(inputs.to(device))\n",
    "              outputs = outputs.detach().cpu()\n",
    "\n",
    "            features.append(outputs)\n",
    "\n",
    "            labels += batch_labels.tolist()\n",
    "            # Use original images for display (already converted to PIL above)\n",
    "            for img in pil_images:\n",
    "                buffered = BytesIO()\n",
    "                img.save(buffered, format=\"PNG\")\n",
    "                encoded_image = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "                images.append(f\"data:image/png;base64,{encoded_image}\")\n",
    "    return torch.cat(features).cpu().numpy(), labels, images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class HDF5Dataset(Dataset):\n",
    "    def __init__(self, datasets_path, file_name, img_key, n_samples, label_key=None):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        for dataset_id, dataset_path in enumerate(datasets_path):\n",
    "            with h5py.File(os.path.join(dataset_path, file_name), 'r') as f:\n",
    "                demo_keys = list(f[\"data\"].keys())[:n_samples]\n",
    "                images = np.concatenate([f[\"data\"][k][\"obs\"][img_key] for k in demo_keys], axis=0)\n",
    "                images = images.transpose(0, 3, 1, 2)\n",
    "                self.data.append(images)\n",
    "                if label_key is not None:\n",
    "                    labels = np.concatenate([f[\"data\"][k][\"obs\"][label_key] for k in demo_keys])\n",
    "                    unique_strings, encoded_array = np.unique(labels, return_inverse=True)\n",
    "                    self.labels.extend(encoded_array)\n",
    "                else:\n",
    "                    self.labels.extend([dataset_id] * images.shape[0])\n",
    "        self.data = np.concatenate(self.data, axis=0)\n",
    "        self.labels = np.array(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "\n",
    "datasets_path = [\n",
    "    # \"/home/weirdlab/Projects/polymetis_franka/data/0520_redblock_400_blocking\",\n",
    "    # \"/home/weirdlab/Projects/polymetis_franka/data/pick_red_cube_rnd_1k_dr\",\n",
    "    \"/home/weirdlab/Projects/polymetis_franka/data/pick_language_cube_500_blocking\"\n",
    "]\n",
    "file_name = \"demos.hdf5\"\n",
    "img_key = \"front_rgb\"\n",
    "n_samples = 100\n",
    "\n",
    "dataset = HDF5Dataset(datasets_path, file_name, img_key, n_samples, label_key=\"language_instruction\")\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for img, label in dataloader:\n",
    "    print(img.shape, label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 198/198 [00:19<00:00, 10.17it/s]\n"
     ]
    }
   ],
   "source": [
    "gpu_id = 0\n",
    "device = torch.device(\"cuda:\" + str(gpu_id) if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "embedding = \"clip\" # \"r3m\", \"in1k\"\n",
    "# n_samples = 1000\n",
    "# batch_size = 1024\n",
    "\n",
    "if embedding == \"in1k\":\n",
    "  model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "  model.to(device)\n",
    "  model.eval()\n",
    "elif embedding == \"r3m\":\n",
    "  from r3m import load_r3m\n",
    "  model = load_r3m(\"resnet18\") # resnet18, resnet34, resnet50\n",
    "  model.to(device)\n",
    "  model.eval()\n",
    "elif embedding == \"clip\":\n",
    "  import clip\n",
    "  model, preprocess = clip.load('ViT-B/32', device=device)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# full_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# if n_samples is not None:\n",
    "#   subset_indices = list(range(n_samples))\n",
    "#   dataset = Subset(full_dataset, subset_indices)\n",
    "# else:\n",
    "#   dataset = full_dataset\n",
    "\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "features, labels, images = get_features_and_images(model, dataloader, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7675b75ab0b74faab6c3838f5f434909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=30, description='perplexity', max=50, min=1), Output()), _dom_classes=('…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.visualize_tsne(perplexity)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "def visualize_tsne(perplexity):\n",
    "    start = time.time()\n",
    "    print(\"Computing embeddings...\")\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, learning_rate=10)\n",
    "    X_tsne = tsne.fit_transform(features)\n",
    "    print(f\"Done. Took {time.time()-start}\")\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], c=labels, cmap='tab10', legend='full')\n",
    "    plt.title(f'2D Visualization of High Dimensional Data with Perplexity: {perplexity}')\n",
    "    plt.xlabel('Component 1')\n",
    "    plt.ylabel('Component 2')\n",
    "    plt.show()\n",
    "\n",
    "# Widget to adjust the perplexity\n",
    "interact(visualize_tsne, perplexity=IntSlider(min=1, max=50, step=1, value=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing embeddings...\n",
      "Done. Took 5.343237638473511\n"
     ]
    }
   ],
   "source": [
    "from jupyter_dash import JupyterDash\n",
    "from dash import dcc, html, Input, Output, State, no_update\n",
    "\n",
    "# App layout\n",
    "app = JupyterDash(__name__)\n",
    "app.layout = html.Div([\n",
    "    dcc.Graph(id=\"tsne-graph\"),\n",
    "    dcc.Slider(\n",
    "        id='perplexity-slider',\n",
    "        min=1,\n",
    "        max=50,\n",
    "        value=30,  # Default value\n",
    "        marks={str(i): str(i) for i in range(1, 51, 1)},\n",
    "        step=1\n",
    "    ),\n",
    "    dcc.Tooltip(id=\"graph-tooltip\"),\n",
    "])\n",
    "\n",
    "# Callback to update t-SNE plot based on perplexity slider\n",
    "@app.callback(\n",
    "    Output(\"tsne-graph\", \"figure\"),\n",
    "    Input(\"perplexity-slider\", \"value\")\n",
    ")\n",
    "def update_tsne(perplexity):\n",
    "\n",
    "    start = time.time()\n",
    "    print(\"Computing embeddings...\")\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, learning_rate=10)\n",
    "    X_tsne = tsne.fit_transform(features)\n",
    "    print(f\"Done. Took {time.time()-start}\")\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"x\": X_tsne[:, 0],\n",
    "        \"y\": X_tsne[:, 1],\n",
    "        \"label\": labels,\n",
    "        \"image\": images\n",
    "    })\n",
    "\n",
    "    fig = go.Figure(data=[\n",
    "        go.Scatter(\n",
    "            x=df[\"x\"],\n",
    "            y=df[\"y\"],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(colorscale='Viridis', color=df[\"label\"], size=10, opacity=0.8),\n",
    "            hoverinfo='none'\n",
    "        )\n",
    "    ])\n",
    "    fig.update_layout(plot_bgcolor='rgba(255,255,255,0.1)')\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Callback to display hover tooltip\n",
    "@app.callback(\n",
    "    Output(\"graph-tooltip\", \"show\"),\n",
    "    Output(\"graph-tooltip\", \"bbox\"),\n",
    "    Output(\"graph-tooltip\", \"children\"),\n",
    "    Input(\"tsne-graph\", \"hoverData\"),\n",
    ")\n",
    "def display_hover(hoverData):\n",
    "    if hoverData is None:\n",
    "        return False, no_update, no_update\n",
    "    pt = hoverData['points'][0]\n",
    "    bbox = pt[\"bbox\"]\n",
    "    num = pt[\"pointIndex\"]\n",
    "    img_src = images[num]\n",
    "\n",
    "    children = [\n",
    "        html.Div([\n",
    "            html.Img(src=img_src, style={\"width\": \"100%\"}),\n",
    "            html.H3(f\"Label: {labels[num]}\", style={\"color\": \"darkblue\"})\n",
    "        ], style={'width': '200px', 'white-space': 'normal'})\n",
    "    ]\n",
    "\n",
    "    return True, bbox, children\n",
    "\n",
    "\n",
    "app.run_server(mode='jupyterlab')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
